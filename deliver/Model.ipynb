{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.sys.path.append('../src')\n",
    "from helpers import resize_to_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(os.path.relpath('../data'))\n",
    "image_dir = os.path.abspath(os.path.relpath('../doc/images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTCHA_IMAGES_FOLDER = \"../data/samples\"\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(CAPTCHA_IMAGES_FOLDER):\n",
    "    # Load the image and convert it to grayscale\n",
    "\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Grab the labels\n",
    "    label = image_file.split(os.path.sep)[-1].split('.')[-2]\n",
    "\n",
    "    # Add the image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(data, labels, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_images(data, label):\n",
    "    # Otsu threashold\n",
    "    data_pre = []\n",
    "    for e in data:\n",
    "        ret, th = cv2.threshold(e, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        dilation = cv2.dilate(th, kernel, iterations=1)\n",
    "        erosion = cv2.erode(dilation, kernel, iterations=1)\n",
    "        data_pre.append(erosion)\n",
    "    \n",
    "    # K-means\n",
    "    data_pts = []\n",
    "    for e in data_pre:\n",
    "        data_pts.append(np.where(e == 0))\n",
    "    data_pts = np.array(data_pts)\n",
    "    \n",
    "    X = []\n",
    "    thres = 3\n",
    "    for e in data_pts:\n",
    "        x = (np.vstack((e[1],np.flip(e[0])))).T\n",
    "        l = []\n",
    "        # Discard columns with less than thres points\n",
    "        for i in range(200):\n",
    "            if len(x[x[:,0] == i]) > thres:\n",
    "                for f in x[x[:,0] == i]:\n",
    "                    l.append(f)\n",
    "        x = np.array(l)\n",
    "        X.append(x)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Projection in x-axis\n",
    "    X_proj = [x[:,0].reshape(-1,1) for x in X]\n",
    "    \n",
    "    # Find clusters in projected data\n",
    "    y_kmeans_proj = []\n",
    "    centers_kmeans_proj = []\n",
    "    for i, x in enumerate(X_proj):\n",
    "        kmeans = KMeans(n_clusters=5)#, init=np.array([(i*200/6.0, 25) for i in range(1,6)]))\n",
    "        kmeans.fit(x)\n",
    "        centers_kmeans_proj.append(kmeans.cluster_centers_)\n",
    "        y_kmeans_proj.append(kmeans.predict(x))\n",
    "\n",
    "    centers = [np.sort(e, axis=0) for e in centers_kmeans_proj]\n",
    "    data_chars = []\n",
    "    for i, e in enumerate(data_pre):\n",
    "        chars = []\n",
    "        for j in range(5):\n",
    "            chars.append(e[:,int(centers[i][j]-21):int(centers[i][j]+21)])\n",
    "        data_chars.append(chars)\n",
    "        \n",
    "    return data_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_train_dir = '../data/letters/train'\n",
    "\n",
    "data_chars = create_images(X_train, y_train)\n",
    "\n",
    "if not(os.path.isdir(''.join((letters_train_dir)))):\n",
    "    os.mkdir(''.join((letters_train_dir)))\n",
    "\n",
    "for i,e in enumerate(data_chars):\n",
    "    for j in range(5):\n",
    "        if not(os.path.isdir(''.join((letters_train_dir,'/',y_train[i],'/')))):\n",
    "            os.mkdir(''.join((letters_train_dir,'/',y_train[i],'/')))\n",
    "        cv2.imwrite(''.join((letters_train_dir,'/',y_train[i],'/',str(j),'-',y_train[i][j],'.png')),e[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_test_dir = '../data/letters/test'\n",
    "\n",
    "data_chars_test = create_images(X_test, y_test)\n",
    "\n",
    "if not(os.path.isdir(''.join((letters_test_dir)))):\n",
    "    os.mkdir(''.join((letters_test_dir)))\n",
    "\n",
    "for i,e in enumerate(data_chars_test):\n",
    "    for j in range(5):\n",
    "        if not(os.path.isdir(''.join((letters_test_dir,'/',y_test[i],'/')))):\n",
    "            os.mkdir(''.join((letters_test_dir,'/',y_test[i],'/')))\n",
    "        cv2.imwrite(''.join((letters_test_dir,'/',y_test[i],'/',str(j),'-',y_test[i][j],'.png')),e[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_IMAGES_FOLDER = letters_train_dir\n",
    "\n",
    "# initialize the data and labels\n",
    "data_l_train = []\n",
    "labels_l_train = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 28x28 pixel box\n",
    "    image = resize_to_fit(image, 32, 32)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras happy\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-1].split('.')[-2].split('-')[1]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data_l_train.append(image)\n",
    "    labels_l_train.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_IMAGES_FOLDER = letters_test_dir\n",
    "\n",
    "# initialize the data and labels\n",
    "data_l_test = []\n",
    "labels_l_test = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in sorted(paths.list_images(LETTER_IMAGES_FOLDER)):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the letter so it fits in a 28x28 pixel box\n",
    "    image = resize_to_fit(image, 32, 32)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras happy\n",
    "    image = np.expand_dims(image, axis=2)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-1].split('.')[-2].split('-')[1]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data_l_test.append(image)\n",
    "    labels_l_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340, 32, 32, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "X_l_train = np.array(data_l_train, dtype=\"float\") / 255.0\n",
    "X_l_test = np.array(data_l_test, dtype=\"float\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "le = LabelEncoder().fit(np.array((labels_l_train + labels_l_test)))\n",
    "y_l_train = le.transform(np.array(labels_l_train))\n",
    "y_l_test = le.transform(np.array(labels_l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 100\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l_train_t = (torch.from_numpy(X_l_train).float().transpose(1,3)).transpose(2,3)\n",
    "y_l_train_t = torch.from_numpy(y_l_train).long()\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_l_train_t, y_l_train_t)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=round(batch_size_train), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_l_test_t = (torch.from_numpy(X_l_test).float().transpose(1,3)).transpose(2,3)\n",
    "y_l_test_t = torch.from_numpy(y_l_test).long()\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(X_l_test_t, y_l_test_t)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(400, 340)\n",
    "        self.fc2 = nn.Linear(340, 19)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 400)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return F.log_softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, v=False):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.nll_loss(output, target, reduction='mean')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            if v:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        torch.save(net.state_dict(), 'model.pth')\n",
    "        torch.save(optimizer.state_dict(), 'optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = net(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 6.6337, Accuracy: 74/1340 (5%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 5.1017, Accuracy: 936/1340 (69%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.6631, Accuracy: 1153/1340 (86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4741, Accuracy: 1163/1340 (86%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.4315, Accuracy: 1200/1340 (89%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2658, Accuracy: 1220/1340 (91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.3556, Accuracy: 1227/1340 (91%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2541, Accuracy: 1239/1340 (92%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.2015, Accuracy: 1255/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1299, Accuracy: 1259/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1523, Accuracy: 1265/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1453, Accuracy: 1258/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0992, Accuracy: 1262/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1191, Accuracy: 1268/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0765, Accuracy: 1269/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.1048, Accuracy: 1262/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0936, Accuracy: 1257/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0529, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0373, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0483, Accuracy: 1270/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0373, Accuracy: 1269/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0502, Accuracy: 1272/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0618, Accuracy: 1247/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0276, Accuracy: 1266/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0226, Accuracy: 1270/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0136, Accuracy: 1275/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0042, Accuracy: 1281/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9971, Accuracy: 1279/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9976, Accuracy: 1275/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0150, Accuracy: 1270/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9979, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0054, Accuracy: 1268/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0074, Accuracy: 1266/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9902, Accuracy: 1274/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9827, Accuracy: 1279/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9889, Accuracy: 1278/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9890, Accuracy: 1279/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9909, Accuracy: 1277/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9939, Accuracy: 1280/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9777, Accuracy: 1278/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9864, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0026, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9996, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9841, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0056, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0505, Accuracy: 1253/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9805, Accuracy: 1279/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9811, Accuracy: 1278/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9802, Accuracy: 1280/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0560, Accuracy: 1267/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9881, Accuracy: 1275/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0165, Accuracy: 1272/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0211, Accuracy: 1270/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9892, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0052, Accuracy: 1267/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9889, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0041, Accuracy: 1268/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9965, Accuracy: 1274/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9974, Accuracy: 1274/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0100, Accuracy: 1277/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0177, Accuracy: 1269/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9982, Accuracy: 1279/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0151, Accuracy: 1275/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9988, Accuracy: 1278/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0142, Accuracy: 1277/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0042, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0307, Accuracy: 1263/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0037, Accuracy: 1286/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0216, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0152, Accuracy: 1275/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0517, Accuracy: 1264/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0121, Accuracy: 1268/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0254, Accuracy: 1269/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0427, Accuracy: 1284/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0379, Accuracy: 1267/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0187, Accuracy: 1278/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 3.9964, Accuracy: 1278/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0270, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0359, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0217, Accuracy: 1272/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0532, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0334, Accuracy: 1269/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0497, Accuracy: 1265/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0435, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0357, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0250, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0667, Accuracy: 1267/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0228, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0297, Accuracy: 1274/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0468, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0307, Accuracy: 1276/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0441, Accuracy: 1267/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0320, Accuracy: 1279/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0231, Accuracy: 1273/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0236, Accuracy: 1274/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0628, Accuracy: 1252/1340 (93%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0572, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0244, Accuracy: 1280/1340 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0620, Accuracy: 1271/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0458, Accuracy: 1272/1340 (94%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 4.0605, Accuracy: 1270/1340 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_IMAGES_FOLDER = '../data/letters/test'\n",
    "\n",
    "\n",
    "# initialize the data and labels\n",
    "data_test = []\n",
    "labels_test = []\n",
    "\n",
    "# loop over the input images\n",
    "for d in os.listdir(LETTER_IMAGES_FOLDER):\n",
    "    aux = []\n",
    "    for image_file in sorted(paths.list_images(LETTER_IMAGES_FOLDER + '/' + d)):\n",
    "        # Load the image and convert it to grayscale\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Resize the letter so it fits in a 28x28 pixel box\n",
    "        image = resize_to_fit(image, 32, 32)\n",
    "\n",
    "        # Add a third channel dimension to the image to make Keras happy\n",
    "        image = np.expand_dims(image, axis=2)\n",
    "        aux.append(image)\n",
    "        \n",
    "\n",
    "\n",
    "    data_test.append(aux)\n",
    "    labels_test.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 32, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = []\n",
    "\n",
    "for i, e in enumerate(labels_test):\n",
    "    x = np.array(data_test[i], dtype=\"float\") / 255.0\n",
    "    x = (torch.from_numpy(x).float().transpose(1,3)).transpose(2,3)\n",
    "    out = net(x)\n",
    "    pred = out.data.max(1, keepdim=True)[1]\n",
    "    y = (''.join([e for e in le.inverse_transform(np.ravel(pred))]))\n",
    "    pred_test.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n3c6y d3c8y\n",
      "ny52p ny5dp\n",
      "675g3 675p3\n",
      "g4cx3 g3ex3\n",
      "e2d5n 22d5n\n",
      "5pcbw xbcbx\n",
      "2g783 2g783\n",
      "3b7xf 3d7bd\n",
      "f2fge f2fge\n",
      "7ynge 7gnge\n",
      "w7652 37d52\n",
      "864dn 664dn\n",
      "5ng6e 5ng6e\n",
      "g7w6y g7wxw\n",
      "6cn48 62nb3\n",
      "6byng 6bxwg\n",
      "gpbwn dpbyd\n",
      "wm47f wm47f\n",
      "ybncw wbncw\n",
      "532fm 5325m\n",
      "2n78f 2n73f\n",
      "n2y3x dy3cx\n",
      "3cpwb 3cpwb\n",
      "dw6mn dw6mn\n",
      "5n243 5n245\n",
      "4n2yg 4n2yg\n",
      "573e8 573d8\n",
      "n7g4f n7g4f\n",
      "8y63f 8y63f\n",
      "7y3eb ny3nn\n",
      "2xcbn 2xc2n\n",
      "658xc 658xe\n",
      "x2f4e x277e\n",
      "bmy2x nmy2x\n",
      "nbwpg nbwpn\n",
      "xw7nc yw7ny\n",
      "cpwe6 c6we6\n",
      "bgp68 bgb48\n",
      "pxcn8 wxcn8\n",
      "g7n8p 4743p\n",
      "dc45x de45x\n",
      "gc83b gc83b\n",
      "gw468 gw468\n",
      "25m6p 25m6p\n",
      "7g4nf 7g3nf\n",
      "76y5f 76y6f\n",
      "3m26n dn26n\n",
      "bgxcd bgxcd\n",
      "b7men b2nen\n",
      "fbp2c fbp2c\n",
      "nmx8p dmx8p\n",
      "d4n82 d4n82\n",
      "p6m48 p6mn8\n",
      "yw75g yyg5g\n",
      "c7gb3 c7gb3\n",
      "cfm34 efx34\n",
      "bg2y8 ng2gw\n",
      "y5g87 y5g87\n",
      "3n4wx nn4wx\n",
      "f576m w7e6m\n",
      "4pg7m 4egem\n",
      "4gb3f 4gb3f\n",
      "cfw3n dw3nn\n",
      "nwc57 nnn57\n",
      "6dnx7 6dmx7\n",
      "p8w5f p8wwf\n",
      "3p4nm 3p4nn\n",
      "3w7g2 387g2\n",
      "8f4n4 x44n4\n",
      "2bf7m 33f7m\n",
      "43mn5 43mn5\n",
      "4e2y7 7e2y7\n",
      "x4nyg xemyg\n",
      "exycn exycn\n",
      "265cm 8684m\n",
      "y48c3 y48c3\n",
      "fdxpn mdxpn\n",
      "dxy28 dnmd8\n",
      "p8ngx p8ngx\n",
      "64b3p 64b3p\n",
      "4gc2f bnc2f\n",
      "w4cnb w4cnn\n",
      "c6745 c6745\n",
      "mc5de pcede\n",
      "pgn5e pgm2e\n",
      "3ebmw 3ebpw\n",
      "422wn f228n\n",
      "7gmf3 7gmf3\n",
      "mgw3n mgw3n\n",
      "8w34m 8n34n\n",
      "7mwp4 77wp4\n",
      "fc6xb fc6xb\n",
      "mye68 mye68\n",
      "72xn3 728n8\n",
      "cped3 cpe63\n",
      "25egp 25egp\n",
      "gc824 yy824\n",
      "e3gfn e3ndn\n",
      "xyymw xyyyw\n",
      "6pwcn 6pwcn\n",
      "bpwd7 bpwd7\n",
      "6fg4c 6fg8c\n",
      "m457d m457d\n",
      "mgdwb mgdwb\n",
      "3mydn 3mxdn\n",
      "cwgyx cwgyx\n",
      "5xc43 5gcd3\n",
      "28x47 28x47\n",
      "mdg7y ndg2b\n",
      "d8xcn d8xcn\n",
      "f35xp f35xp\n",
      "emd4w ecd4w\n",
      "64c82 64m82\n",
      "p2m7w p2x7x\n",
      "48gnd g8gnd\n",
      "x4y75 x4gg5\n",
      "6pwyd ppwyd\n",
      "8gb67 8db67\n",
      "p8c24 p8c24\n",
      "nmy4e nnp4e\n",
      "gwn53 gwn53\n",
      "gc2f7 gc277\n",
      "8562n 85622\n",
      "cn4p6 cnmnn\n",
      "xc68n xc68n\n",
      "bgfb3 bdbb3\n",
      "m87wp mp7wp\n",
      "6gnm3 6gnm3\n",
      "5wnd3 3wnd3\n",
      "7c24w fc2ff\n",
      "6e2bg 6e2dg\n",
      "f62bw 662bw\n",
      "pg2wm pg2pm\n",
      "e6b7y e6b7y\n",
      "pe4xn pe4xn\n",
      "wcg2m mmg2m\n",
      "nyxb7 npxb7\n",
      "2w4y7 2w4y7\n",
      "f5yeb 3ye2e\n",
      "n4bfm n4b4m\n",
      "gbny3 dbny3\n",
      "n7ebx n7ebx\n",
      "w4nfx w4nfx\n",
      "fxpw3 fxpw3\n",
      "5exbp 5expp\n",
      "mxy7w mxyxw\n",
      "w2e8m w2e87\n",
      "pyefb pyefb\n",
      "g8n43 gdng3\n",
      "efgx5 efgx5\n",
      "wne56 wmpmp\n",
      "fg8n4 fg8n4\n",
      "3emy7 3eny7\n",
      "db736 8b735\n",
      "8xwdy xxw44\n",
      "4fcey f7cey\n",
      "mc2bd wc2bd\n",
      "6pmyc 6wnyc\n",
      "8fwe7 dnne7\n",
      "7g4nx 7nnnx\n",
      "7dxy4 7dxbd\n",
      "24fpw 24f6w\n",
      "3xcmn nxcmn\n",
      "bdg84 bdg84\n",
      "7p852 7p852\n",
      "3ygde 3ygde\n",
      "dpxn7 gpxng\n",
      "ygbwe ygfwe\n",
      "4gycb 4gycb\n",
      "87nym 87nym\n",
      "w8py4 w6ny4\n",
      "5bn47 5bnd7\n",
      "xdcn4 xdcn4\n",
      "pwm3n pwmbn\n",
      "57wdp 57wdp\n",
      "cwmny cwmny\n",
      "c84eg cd4eg\n",
      "p7n23 een23\n",
      "58b5m 58b5m\n",
      "8nme3 8npe3\n",
      "g64mf gd4mf\n",
      "5wmf2 5mf7c\n",
      "6mygb 6mygb\n",
      "p57fn p57fn\n",
      "nfbg8 nfbg8\n",
      "f3pxe 63pxe\n",
      "w5cg7 n5cm7\n",
      "6y5fn w52fn\n",
      "4gcex 4ycex\n",
      "yge7c yge7c\n",
      "3xnfy 56m6y\n",
      "e46yw e46yw\n",
      "gwc3n gnc3n\n",
      "pxne8 pxne8\n",
      "xwebn x8e8n\n",
      "pcn7f pcm7f\n",
      "n2c85 n2c85\n",
      "xfmey xxney\n",
      "fnew6 nn6w6\n",
      "mbcg6 nbcgb\n",
      "4cn7p 4cn7b\n",
      "82pxg n2gmg\n",
      "m2xfw m3wfw\n",
      "y3c58 y3c58\n",
      "yp87n pp87n\n",
      "gy8xb gy8xb\n",
      "gnfx3 nnfx3\n",
      "x3deb x3deb\n",
      "gy8cn gm6nn\n",
      "f6yw8 f6ww8\n",
      "8bbm4 8bbm4\n",
      "efm6p e6m6p\n",
      "f85y3 f85y3\n",
      "8632p b6f2p\n",
      "y7x8p y7x8p\n",
      "78fbg f8f8g\n",
      "y32wc y32yy\n",
      "cpdm3 cndmc\n",
      "3yn7f 3ym7f\n",
      "befmd befbd\n",
      "w6yne w6yne\n",
      "f5cn2 f5cm2\n",
      "gfxn3 gfxcc\n",
      "4753c f753f\n",
      "nfcb5 nfcb5\n",
      "364pw 3nnpw\n",
      "bc8nf bc8nf\n",
      "ewnx8 ewnx8\n",
      "d856m d666m\n",
      "gmf85 gnf85\n",
      "g2fmw g2fnw\n",
      "byc82 byc82\n",
      "xmd5y xnd3y\n",
      "gywxn gpnxn\n",
      "d4ep5 deep5\n",
      "35cnw 5x5nx\n",
      "2x6wm 2x7bm\n",
      "pwd5e pwn5e\n",
      "b5en7 b5nmm\n",
      "7fd8g nfd8g\n",
      "7n6c2 7dgc2\n",
      "d2ycw d2ycw\n",
      "x3bwf x3fwf\n",
      "n3xfg n3xfg\n",
      "2cnf4 2enf4\n",
      "fp762 fp762\n",
      "byfgn byfgn\n",
      "fmpw5 4m2w5\n",
      "8pnf4 8w754\n",
      "p5nce p5nce\n",
      "myfd8 m4fd8\n",
      "n4ey5 d22y5\n",
      "ngdc4 mn5c4\n",
      "y2m76 wwmn6\n",
      "my84e my84e\n",
      "467d5 467d5\n",
      "dbpme ddcne\n",
      "nbpye nbp3e\n",
      "pm73w pmd3w\n",
      "bcey3 fcey3\n",
      "w2yp7 w2yp7\n",
      "n3bw6 n3bm6\n",
      "n265y n265y\n",
      "bp2d4 bp2d4\n",
      "npyf8 nxxf8\n",
      "54f7g x4f7g\n",
      "n3d6x n3m6x\n",
      "g8yw4 gegw4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for e, f in zip(pred_test, labels_test):\n",
    "    print(e, f)\n",
    "    if e == f:\n",
    "        correct += 1\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3246268656716418"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/len(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(y_hat, y, n_classes):\n",
    "    ans = np.zeros((n_classes, n_classes))\n",
    "    for i, (e, f) in enumerate(zip(y, y_hat)):\n",
    "        ans[classes.index(e)][int(f)] += 1\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3', '4', '5', '6', '7', '8', 'b', 'c', 'd', 'e', 'f', 'g', 'm', 'n', 'p', 'w', 'x', 'y']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'d3c8y' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2069ab92ccdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-922cd5c62d02>\u001b[0m in \u001b[0;36mconfusion\u001b[0;34m(y_hat, y, n_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'd3c8y' is not in list"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "cm = confusion(pred_test, labels_test, len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
